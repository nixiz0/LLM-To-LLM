# LLM To LLM (on Windows)

Fun test to put 2 Local LLMs into discussions with each other.


## Installation

- Run the **start-script.bat** to install the env and librairies

[=**Be careful here we use CUDA 11.8**=]

[=**Be careful to have installed Microsoft C++ build tools on your computer**=]

- Install Ollama **https://ollama.com/download**

- Pull the model you want (here i used llama3)

- You can modify the main.py to change models and language


## Author

- [@nixiz0](https://github.com/nixiz0)